# cs577-emoji-embeddings

### Things to make sure to do for final report
- measuring accruacy in n-gram using emoji-pastas and our embeddings, cosine-distance of related emojis?
- [LOOK INTO MORE] embedding objective: measuring accruacy in n-gram using emoji-pastas and our embeddings vs someone else's word embeddings
- [LOOK INTO MORE] context beyond a single word: n-gram model or maybe move to LSTM/RNN?

### For Monday :
- Get most preprocessing done
  + Filtering out requests
  + Filtering out innapropriate?
  + Parsing logic like seperating words/emojis
  + Filter out posts smaller than (10?) words and less than 20%
  + PyTorch data stuff
- Simon: 
  + Data cleaning and preprocessing
- Aidan:
  + Pytorch data setup? Works with GPU so we don't go insane
